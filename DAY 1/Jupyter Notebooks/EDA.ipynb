{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) Overview\n",
    "\n",
    "**What is EDA?**  \n",
    "Exploratory Data Analysis (EDA) is the process of analyzing data sets to summarize their main characteristics, often using visual methods. It is a critical step in understanding the structure of your data, uncovering patterns, spotting anomalies, and testing hypotheses before applying formal modeling techniques.\n",
    "\n",
    "**Dataset Used:**  \n",
    "We are working with the **Iris dataset**, a well-known dataset in data science that includes measurements (sepal length, sepal width, petal length, petal width) for 150 iris flowers, along with their species classification. This dataset is ideal for demonstrating various EDA techniques because of its simplicity and multiple variable types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of Central Tendency, Variance, and Standard Deviation\n",
    "  \n",
    "These measures help summarize and describe the main features of a dataset:\n",
    "- **Central Tendency:** Gives a central value for the data.\n",
    "- **Variability:** Shows how much the data values differ from the central value.\n",
    "\n",
    "**Formulas:**\n",
    "\n",
    "1. **Mean (Average):**\n",
    "   $$ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i $$\n",
    "   *Gives the average value of all observations.*\n",
    "\n",
    "2. **Median:**  \n",
    "   The middle value when the data is sorted.  \n",
    "   *Useful when the data has outliers, as it is more robust than the mean.*\n",
    "\n",
    "3. **Mode:**  \n",
    "   The most frequently occurring value in the dataset.\n",
    "\n",
    "4. **Variance:**  \n",
    "   $$ \\sigma^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 $$\n",
    "   *Measures the spread of the data around the mean.*\n",
    "\n",
    "5. **Standard Deviation:**  \n",
    "   $$ \\sigma = \\sqrt{\\sigma^2} $$\n",
    "   *Provides the dispersion in the same units as the data.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "# Display first few rows of the dataset\n",
    "print(\"Iris Dataset Head:\")\n",
    "print(iris.head())\n",
    "\n",
    "# Calculate central tendency measures for the 'sepal_length' column\n",
    "mean_val = iris['sepal_length'].mean()\n",
    "median_val = iris['sepal_length'].median()\n",
    "mode_val = iris['sepal_length'].mode()[0]\n",
    "\n",
    "print(\"\\nMeasures for 'sepal_length':\")\n",
    "print(\"Mean:\", mean_val)\n",
    "print(\"Median:\", median_val)\n",
    "print(\"Mode:\", mode_val)\n",
    "\n",
    "# Calculate variance and standard deviation\n",
    "variance_val = iris['sepal_length'].var()\n",
    "std_dev = iris['sepal_length'].std()\n",
    "\n",
    "print(\"\\nVariance:\", variance_val)\n",
    "print(\"Standard Deviation:\", std_dev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    " \n",
    "Resampling is used to change the frequency of time-series data. It helps in smoothing out noise or preparing data for specific time-based analyses (e.g., converting daily data to weekly averages).\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Downsampling:** Aggregating data from a higher frequency to a lower frequency (e.g., daily to weekly).\n",
    "- **Upsampling:** Increasing the frequency of data points (e.g., daily to hourly), usually followed by an imputation step.\n",
    "\n",
    "*In the next cell, we simulate a time-series and apply downsampling to compute weekly averages.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a time-series dataset with a daily frequency over 60 days\n",
    "date_range = pd.date_range(start='2023-01-01', periods=60, freq='D')\n",
    "ts_data = pd.DataFrame({\n",
    "    'date': date_range,\n",
    "    'value': np.random.randn(60).cumsum()  # cumulative sum to simulate a trend\n",
    "})\n",
    "ts_data.set_index('date', inplace=True)\n",
    "\n",
    "# Downsample the data to a weekly frequency using the mean\n",
    "weekly_data = ts_data.resample('W').mean()\n",
    "\n",
    "# Plot the original and resampled data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ts_data.index, ts_data['value'], label='Daily Data')\n",
    "plt.plot(weekly_data.index, weekly_data['value'], 'o-', label='Weekly Mean')\n",
    "plt.title(\"Time Series Resampling: Daily to Weekly\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "  \n",
    "Missing data can distort the results of your analysis. Handling missing values ensures that your conclusions are based on complete information. Common methods include:\n",
    "- **Detection:** Identifying missing entries.\n",
    "- **Removal:** Dropping rows/columns with missing values.\n",
    "- **Imputation:** Filling in missing values using statistical methods.\n",
    "\n",
    "**Imputation Example:**  \n",
    "Filling missing values with the mean:  \n",
    "$$ x_{\\text{imputed}} = \\bar{x} $$  \n",
    "This method replaces missing values with the average of the column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce missing values into the Iris dataset (for demonstration)\n",
    "iris_missing = iris.copy()\n",
    "# Randomly assign NaN to 5% of the 'petal_length' values\n",
    "np.random.seed(42)\n",
    "missing_indices = np.random.choice(iris_missing.index, size=int(0.05 * len(iris_missing)), replace=False)\n",
    "iris_missing.loc[missing_indices, 'petal_length'] = np.nan\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing Values in Each Column:\")\n",
    "print(iris_missing.isnull().sum())\n",
    "\n",
    "# Impute missing values with the mean of the column\n",
    "iris_missing['petal_length'] = iris_missing['petal_length'].fillna(iris_missing['petal_length'].mean())\n",
    "print(\"\\nAfter imputation, missing values:\")\n",
    "print(iris_missing.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Atypical Values (Outliers)\n",
    "  \n",
    "Outliers can indicate variability in measurement or experimental errors. Detecting outliers helps in understanding data quality and sometimes in deciding whether to exclude such points from further analysis.\n",
    "\n",
    "**IQR (Interquartile Range) Method:**\n",
    "\n",
    "1. Compute Q1 (25th percentile) and Q3 (75th percentile).\n",
    "2. Calculate IQR:\n",
    "   $$ \\text{IQR} = Q3 - Q1 $$\n",
    "3. Determine outlier boundaries:\n",
    "   $$ \\text{Lower Bound} = Q1 - 1.5 \\times \\text{IQR} $$\n",
    "   $$ \\text{Upper Bound} = Q3 + 1.5 \\times \\text{IQR} $$\n",
    "Any value outside these bounds is considered a potential outlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the IQR method to detect outliers in 'sepal_width'\n",
    "Q1 = iris['sepal_width'].quantile(0.25)\n",
    "Q3 = iris['sepal_width'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier bounds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Find outliers\n",
    "outliers = iris[(iris['sepal_width'] < lower_bound) | (iris['sepal_width'] > upper_bound)]\n",
    "print(\"Outliers in 'sepal_width':\")\n",
    "print(outliers[['sepal_width']])\n",
    "\n",
    "# Plot a boxplot for visualization\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(x=iris['sepal_width'])\n",
    "plt.title(\"Boxplot of Sepal Width\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation and Covariance\n",
    "  \n",
    "These measures help us understand the relationship between variables:\n",
    "- **Covariance:** Indicates the direction of the linear relationship between variables.\n",
    "- **Correlation:** Standardizes covariance to a range between -1 and 1, indicating both the strength and direction of the relationship.\n",
    "\n",
    "**Formulas:**\n",
    "\n",
    "1. **Covariance:**  \n",
    "   $$ \\text{cov}(X,Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) $$\n",
    "\n",
    "2. **Correlation (Pearson):**  \n",
    "   $$ r = \\frac{\\sum_{i=1}^{n} (x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2 \\sum_{i=1}^{n}(y_i-\\bar{y})^2}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns from the dataset\n",
    "numeric_iris = iris.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate covariance matrix for the numeric columns\n",
    "cov_matrix = numeric_iris.cov()\n",
    "print(\"Covariance Matrix:\")\n",
    "print(cov_matrix)\n",
    "\n",
    "# Calculate correlation matrix for the numeric columns\n",
    "corr_matrix = numeric_iris.corr()\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(corr_matrix)\n",
    "\n",
    "# Visualize the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cov_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Covariance Matrix Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "  \n",
    "A confusion matrix is a tool used to evaluate the performance of a classification model by summarizing the correct and incorrect predictions.\n",
    "\n",
    "**Components:**\n",
    "- **True Positives (TP):** Correctly predicted positive cases.\n",
    "- **True Negatives (TN):** Correctly predicted negative cases.\n",
    "- **False Positives (FP):** Incorrectly predicted positive cases.\n",
    "- **False Negatives (FN):** Incorrectly predicted negative cases.\n",
    "\n",
    "*While there isn't a specific formula for a confusion matrix, its structure is crucial for calculating metrics such as accuracy, precision, recall, and F1-score.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Simulate true labels and predicted labels for a binary classification problem\n",
    "true_labels = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n",
    "predicted_labels = np.array([0, 0, 0, 1, 0, 1, 1, 1, 0, 1])\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Visualize the confusion matrix using a heatmap\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt=\"d\", cbar=False,\n",
    "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "            yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"Actual Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we:\n",
    "- Introduced the concept of EDA and the Iris dataset.\n",
    "- Reviewed key statistical measures (mean, median, mode, variance, standard deviation) with formulas.\n",
    "- Applied resampling techniques to time-series data.\n",
    "- Demonstrated methods to handle missing values.\n",
    "- Detected atypical values (outliers) using the IQR method.\n",
    "- Computed and visualized correlation and covariance to understand relationships between variables.\n",
    "- Built and visualized a confusion matrix to evaluate a classification task.\n",
    "\n",
    "This combination of theory (with formulas) and code helps provide a comprehensive foundation in exploratory data analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
