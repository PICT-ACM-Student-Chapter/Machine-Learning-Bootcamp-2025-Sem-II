{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import math\n",
        "import pandas as pd\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "UcqETlncx9rl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multivariate Linear Regression"
      ],
      "metadata": {
        "id": "ADxxDyD4vmqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_15456_2\"></a>\n",
        "# 2 Problem Statement\n",
        "\n",
        " The training dataset contains three examples with four features (size, bedrooms, floors and, age) shown in the table below.  Note that, unlike the earlier labs, size is in sqft rather than 1000 sqft. This causes an issue, which you will solve in the next lab!\n",
        "\n",
        "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
        "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
        "| 2104            | 5                   | 1                | 45           | 460           |  \n",
        "| 1416            | 3                   | 2                | 40           | 232           |  \n",
        "| 852             | 2                   | 1                | 35           | 178           |  \n",
        "\n",
        "You will build a linear regression model using these values so you can then predict the price for other houses. For example, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old.  \n",
        "\n"
      ],
      "metadata": {
        "id": "viZ7mhU5v0aF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
        "y_train = np.array([460, 232, 178])"
      ],
      "metadata": {
        "id": "NPv_6mJEv5VF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data is stored in numpy array/matrix\n",
        "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
        "print(X_train)\n",
        "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdgEc8Oev7Ok",
        "outputId": "93be904d-85fe-4124-d663-777acec6df26"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X Shape: (3, 4), X Type:<class 'numpy.ndarray'>)\n",
            "[[2104    5    1   45]\n",
            " [1416    3    2   40]\n",
            " [ 852    2    1   35]]\n",
            "y Shape: (3,), y Type:<class 'numpy.ndarray'>)\n",
            "[460 232 178]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_15456_2.2\"></a>\n",
        "## 2.2 Parameter vector w, b\n",
        "\n",
        "* $\\mathbf{w}$ is a vector with $n$ elements.\n",
        "  - Each element contains the parameter associated with one feature.\n",
        "  - in our dataset, n is 4.\n",
        "  - notionally, we draw this as a column vector\n",
        "\n",
        "$$\\mathbf{w} = \\begin{pmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "\\cdots\\\\\n",
        "w_{n-1}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "* $b$ is a scalar parameter.  "
      ],
      "metadata": {
        "id": "UUO2kLP8wJW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b_init = 785.1811367994083\n",
        "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
        "print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFnmIMHQwGL-",
        "outputId": "2b86d18a-2c36-43c0-d358-99d46f3340dc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w_init shape: (4,), b_init type: <class 'float'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_15456_3\"></a>\n",
        "# 3 Model Prediction With Multiple Variables\n",
        "The model's prediction with multiple variables is given by the linear model:\n",
        "\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
        "or in vector notation:\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$\n",
        "where $\\cdot$ is a vector `dot product`\n",
        "\n",
        "To demonstrate the dot product, we will implement prediction using (1) and (2)."
      ],
      "metadata": {
        "id": "O_PiaCocwM91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Single Prediction using vector dot product"
      ],
      "metadata": {
        "id": "-mmA38HKwTIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(x, w, b):\n",
        "    \"\"\"\n",
        "    single predict using linear regression\n",
        "    Args:\n",
        "      x (ndarray): Shape (n,) example with multiple features\n",
        "      w (ndarray): Shape (n,) model parameters\n",
        "      b (scalar):             model parameter\n",
        "\n",
        "    Returns:\n",
        "      p (scalar):  prediction\n",
        "    \"\"\"\n",
        "    p = np.dot(x, w) + b\n",
        "    return p"
      ],
      "metadata": {
        "id": "EwT449LYwJ-l"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_15456_4\"></a>\n",
        "# 4 Compute Cost With Multiple Variables\n",
        "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
        "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$\n",
        "where:\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZXkngflswoPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(X, y, w, b):\n",
        "    \"\"\"\n",
        "    compute cost\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters\n",
        "      b (scalar)       : model parameter\n",
        "\n",
        "    Returns:\n",
        "      cost (scalar): cost\n",
        "    \"\"\"\n",
        "    m = X.shape[0]\n",
        "    cost = 0.0\n",
        "    for i in range(m):\n",
        "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
        "        cost = cost + (f_wb_i - y[i])**2       #scalar\n",
        "    cost = cost / (2 * m)                      #scalar\n",
        "    return cost"
      ],
      "metadata": {
        "id": "ROlKFa7XwqUr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
        "print(f'Cost at optimal w : {cost}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIAIbyCcwtlN",
        "outputId": "3b669bfa-2064-4d4b-a4cb-11c9f1a32c51"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost at optimal w : 1.5578904428966628e-12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_15456_5\"></a>\n",
        "# 5 Gradient Descent With Multiple Variables\n",
        "Gradient descent for multiple variables:\n",
        "\n",
        "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
        "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
        "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
        "\\end{align*}$$\n",
        "\n",
        "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
        "\\end{align}\n",
        "$$\n",
        "* m is the number of training examples in the data set\n",
        "\n",
        "    \n",
        "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n"
      ],
      "metadata": {
        "id": "y7kfp26WyHXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_15456_5.1\"></a>\n",
        "## 5.1 Compute Gradient with Multiple Variables\n",
        "An implementation for calculating the equations (6) and (7) is below. There are many ways to implement this. In this version, there is an\n",
        "- outer loop over all m examples.\n",
        "    - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ for the example can be computed directly and accumulated\n",
        "    - in a second loop over all n features:\n",
        "        - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ is computed for each $w_j$.\n",
        "   "
      ],
      "metadata": {
        "id": "APHUO6IHyK2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters\n",
        "      b (scalar)       : model parameter\n",
        "\n",
        "    Returns:\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "    m,n = X.shape           #(number of examples, number of features)\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):\n",
        "        err = (np.dot(X[i], w) + b) - y[i]\n",
        "        for j in range(n):\n",
        "            dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
        "        dj_db = dj_db + err\n",
        "    dj_dw = dj_dw / m\n",
        "    dj_db = dj_db / m\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "2waLE6rryCxB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute and display gradient\n",
        "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
        "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
        "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwKlaTWtyOkx",
        "outputId": "ae1f2120-dc06-4160-dd33-52053d1b1fdd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dj_db at initial w,b: -1.6739251501955248e-06\n",
            "dj_dw at initial w,b: \n",
            " [-2.72623577e-03 -6.27197263e-06 -2.21745578e-06 -6.92403391e-05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"toc_15456_5.2\"></a>\n",
        "## 5.2 Gradient Descent With Multiple Variables\n",
        "The routine below implements equation (5) above."
      ],
      "metadata": {
        "id": "sDZjwYiNyS-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn theta. Updates theta by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n))   : Data, m examples with n features\n",
        "      y (ndarray (m,))    : target values\n",
        "      w_in (ndarray (n,)) : initial model parameters\n",
        "      b_in (scalar)       : initial model parameter\n",
        "      cost_function       : function to compute cost\n",
        "      gradient_function   : function to compute the gradient\n",
        "      alpha (float)       : Learning rate\n",
        "      num_iters (int)     : number of iterations to run gradient descent\n",
        "\n",
        "    Returns:\n",
        "      w (ndarray (n,)) : Updated values of parameters\n",
        "      b (scalar)       : Updated value of parameter\n",
        "      \"\"\"\n",
        "\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
        "    b = b_in\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db,dj_dw = gradient_function(X, y, w, b)\n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w = w - alpha * dj_dw\n",
        "        b = b - alpha * dj_db\n",
        "\n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:\n",
        "            J_history.append( cost_function(X, y, w, b))\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters / 10) == 0:\n",
        "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
        "\n",
        "    return w, b, J_history #return final w,b and J history for graphing"
      ],
      "metadata": {
        "id": "tZjDSL_DyQP3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize parameters\n",
        "initial_w = np.zeros_like(w_init)\n",
        "initial_b = 0.\n",
        "# some gradient descent settings\n",
        "iterations = 1000\n",
        "alpha = 5.0e-7\n",
        "\n",
        "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
        "                                                    compute_cost, compute_gradient,\n",
        "                                                    alpha, iterations)\n",
        "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
        "m,_ = X_train.shape\n",
        "for i in range(m):\n",
        "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vKEy6Q6yXrn",
        "outputId": "501e434a-e759-482d-ce23-f7fa093d2ced"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration    0: Cost  2529.46   \n",
            "Iteration  100: Cost   695.99   \n",
            "Iteration  200: Cost   694.92   \n",
            "Iteration  300: Cost   693.86   \n",
            "Iteration  400: Cost   692.81   \n",
            "Iteration  500: Cost   691.77   \n",
            "Iteration  600: Cost   690.73   \n",
            "Iteration  700: Cost   689.71   \n",
            "Iteration  800: Cost   688.70   \n",
            "Iteration  900: Cost   687.69   \n",
            "b,w found by gradient descent: -0.00,[ 0.20396569  0.00374919 -0.0112487  -0.0658614 ] \n",
            "prediction: 426.19, target value: 460\n",
            "prediction: 286.17, target value: 232\n",
            "prediction: 171.47, target value: 178\n"
          ]
        }
      ]
    }
  ]
}